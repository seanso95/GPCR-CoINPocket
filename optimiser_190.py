import os
import json
import numpy as np
import pandas as pd
import sys
import getopt
from itertools import combinations
from scipy.optimize import minimize
from math import sqrt

def CalcROC(x):
    """
    call CalcCP() to calculate CP scores
    call SquarifyMatrix() to return square matrix of CP scores
    run weighted AUC calculation
    take 100 - wAUC as return value - minimise this to maximise accuracy

    input argument format: x = [TODO: UPDATE THIS FIELD]
    """
    if isinstance(x, pd.DataFrame):
        cp_frame = x
        cp_scores = SquarifyMatrix(cp_frame, exclude=False)
    else:
        cp_frame = CalcCP(x)
        cp_scores = SquarifyMatrix(cp_frame, exclude=True)

    scaled_auc = ROC_AUC(cp_scores)
    print(f'{scaled_auc}')
    with open(f'{OUT_FOLDER}/{SERIAL}_{EXCL_FOLD}_scores.log', 'a') as myfile:
        myfile.write(f'AUC: {scaled_auc}\n')
    # with open(RESULT_FRAME_FILE, 'a') as f:
    #     f.write('\n')
    #     f.write(', '.join(map(str, vals_to_min)))
    return 100 - scaled_auc


def ROC_AUC(in_df):
    """
    Calculate ROC score and scale it to emphasise early positives
    Function written to mimic ICM implementation

    input: dataframe generated by CalcCP
    output: 100 - scaled AUC (cost function score), used for minimiser
    """
    in_df['simAvgP'] = -in_df['simAvgP']
    in_df = in_df.sort_values(by='simAvgP')

    # account for ranking issues by treating identical scores as group
    dupes = in_df[in_df.duplicated(subset=['simAvgP'], keep=False)]
    grouped_dupes = dupes.groupby('simAvgP')
    if (grouped_dupes['tr'].sum() > 0).any():  # if any group needs averaging
        # average the groups, then replace values in main table
        grouped_avg = grouped_dupes['tr'].mean()
        grouped_avg = grouped_avg[grouped_avg > 0]
        for sim_score, avg_tr in grouped_avg.items():
            in_df.loc[in_df['simAvgP'] == sim_score, 'tr'] = avg_tr

    # calculate the scaled AUC
    nof_pts = len(in_df)
    nof_correct = len(in_df[in_df['tr'] > 0.])
    true_pos = (100 * np.cumsum(in_df['tr'])) / nof_correct
    cum_pts_frac = np.asarray([i for i in range(1, nof_pts + 1)]) / nof_pts
    weighted_points = [100 * sqrt(_) for _ in cum_pts_frac]
    x_increments = np.array(weighted_points) - [0., *weighted_points[0:-1]]
    scaled_auc = (0.01 * sum(true_pos * x_increments))
    return scaled_auc


def SquarifyMatrix(x, exclude=True):
    """
    Turn input matrix/dataframe into a square.

    When exclude is True, the specified EXCL_FOLD is removed from the dataset.
    This is used when training the weights, calculating ROC AUC from 4 of 5
    folds.

    When exclude is False, only EXCL_FOLD is kept. This is used to store a
    record of the 'test' fold AUC for plotting stability of the minimisation.
    """
    cp_scores = x[['pair', 'simAvgP']].copy()

    # convert to square
    tmp_tab = cp_scores.copy()
    tmp_tab['pair'] = (tmp_tab['pair'].str.split('@')
                       .map(lambda x: x[::-1] if x[0] != x[1] else '')
                       .map(lambda x: '@'.join(x)))
    cp_scores = pd.concat([cp_scores, tmp_tab.loc[tmp_tab.pair != '']],
                          ignore_index=True)

    # merge true/false labels
    cp_scores = pd.merge(left=TAB_LABELS,
                         right=cp_scores,
                         left_on='pairix',
                         right_on='pair')

    if exclude:
        # remove excluded fold
        cp_scores = cp_scores.loc[cp_scores.fold != EXCL_FOLD]
    else:
        # keep only specified fold
        cp_scores = cp_scores.loc[cp_scores.fold == EXCL_FOLD]

    return cp_scores

def CalcCP(x):
    """
    Complete rewrite - create superlarge table with indices and perform a single groupby operation
    """

    def Zscore(df):
        """
        Perform Z-score normalisation on column 'sim' of an input DataFrame.
        For use with pd.groupby.DataFrameGroupBy.apply().
        """
        df['same'] = df['i'] - df['j']
        tmp = df[df['same'] < 0]
        mu = tmp['sim'].mean()
        sigma = tmp['sim'].std()
        df.loc[:, 'sim'] = (df['sim'] - mu)/sigma
        return(df.drop('same', axis=1))

    similarities = np.concatenate([x, x, np.ones(20)])
    tmp_score = TAB_PAIRS_LONG.copy()
    tmp_score['weight'] = similarities[tmp_score['seq'].values.astype(int)]
    tmp_score['sim'] = tmp_score['stre'] * tmp_score['weight']
    tmp_score = tmp_score[tmp_score['seq'] > -1]
    tmp_score.reset_index(drop=True, inplace=True)
    tmp_score_grouped = tmp_score[['i', 'j', 'fp', 'sim']].groupby(['fp', 'i', 'j']).sum().reset_index()
    tmp_score_grouped = tmp_score_grouped.groupby('fp').apply(Zscore)
    zdf = tmp_score_grouped.reset_index(drop=True).pivot(index=['i', 'j'],
                                                         columns='fp',
                                                         values='sim').reset_index()
    zdf['simAvgP'] = zdf.drop(['i', 'j'], axis=1).mean(axis=1)
    zdf['pair'] = zdf['i'].astype('str') + '@' + zdf['j'].astype('str')
    return(zdf)


# detect flags: -x aa_pair_index -f fold -t algorithm_type -d output_folder -r true/false
RAND = False
opts, args = getopt.getopt(sys.argv[1:], 'x:f:t:d:r:')
for opt, arg in opts:
    if opt in ['-x']:
        i = int(arg)
    elif opt in ['-f']:
        arg = int(arg)
        if 0 <= arg <= 5:
            EXCL_FOLD = arg
        else:
            sys.exit('error: excluded fold must be between 0 and 4 (or 5 for no folds)')
    elif opt in ['-t']:
        if arg.lower() in ['nelder-mead', 'cobyla']:
            METHOD = arg
        else:
            sys.exit('error: algorithm must be nelder-mead or cobyla')
    elif opt in ['-d']:
        OUT_FOLDER = arg
    elif opt in ['-r']:
        RAND = str(arg).capitalize() == 'True'
    else:
        sys.exit('error: incorrect flag used (-x/-f/-t/-d/-r)')

# load and set up dependencies
IN_FOLDER = './in/'
try:
    # TAB_ALIG = pd.read_pickle(f'{IN_FOLDER}/stacked_alig.pkl.zip')
    # TAB_LABELS = pd.read_pickle(f'{IN_FOLDER}/labels_twi_fold.pkl.zip')
    # TAB_PAIRS = pd.read_pickle(f'{IN_FOLDER}/GPCRpairsalig.pkl.zip')
    TAB_PAIRS_LONG = pd.read_pickle(f'{IN_FOLDER}/aaPair_ij_stre_fp.pkl.zip')
    TAB_LABELS = pd.read_pickle(f'{IN_FOLDER}/labels_twi_fold_ix.pkl.zip')  # ix version needed
except FileNotFoundError:
    sys.exit('Dependencies not found - launch code from cpOptim/')

RESIDUES = ['A', 'C', 'D', 'E', 'F',
            'G', 'H', 'I', 'K', 'L',
            'M', 'N', 'P', 'Q', 'R',
            'S', 'T', 'V', 'W', 'Y']
COMBS = sorted(list(combinations(RESIDUES, 2)))
RES_FRAME      = pd.DataFrame(data=np.array([f'{x}{y}' for x, y in COMBS]), columns=['ij'])
RES_FRAME_R    = pd.DataFrame(data=np.array([f'{y}{x}' for x, y in COMBS]), columns=['ij'])
RES_FRAME_SELF = pd.DataFrame(data=np.array([f'{x}{x}' for x in RESIDUES]), columns=['ij'])

# set file naming system
rand_str = 'rand' if RAND else ''
if i < 190:
    SERIAL = ''.join([RES_FRAME['ij'][i], '_0to1', f'_{rand_str}'])
else:
    SERIAL = ''.join([RES_FRAME['ij'][i - 190], '_1to0', f'_{rand_str}'])

# check if this is a rebooted run; quit if already optimised, continue if not
if os.path.isfile(f'{OUT_FOLDER}/aa_{METHOD}_result{SERIAL}_{EXCL_FOLD}.json') is True:
    with open(f'{OUT_FOLDER}/aa_{METHOD}_result{SERIAL}_{EXCL_FOLD}.json', 'r') as file:
        prev = json.load(file)
    if prev['success'] == 'True':
        sys.exit('Previous run was a successful optimisation')
    else:
        prev_simplex = np.array(prev['final_simplex'][0])
else:
    prev_simplex = None

# set up optimiser parameters
MAX_ITER = 1000
if RAND:
    if os.path.isfile(f'{OUT_FOLDER}/{SERIAL}_entropy.txt') is True:
        with open(f'{OUT_FOLDER}/{SERIAL}_entropy.txt', 'r') as f:
            SEED = np.random.SeedSequence(int(f.readline()))
    else:
        SEED = np.random.SeedSequence()
        with open(f'{OUT_FOLDER}/{SERIAL}_entropy.txt', 'w') as f:
            f.write(str(SEED.entropy))
    BIT_GEN = np.random.default_rng(SEED)
    if i < 190:
        vals_to_min = BIT_GEN.uniform(low=0, high=0.1, size=len(RES_FRAME))
        # vals_to_min[i] = 1.0
        vals_to_min[i] = BIT_GEN.uniform(low=0.9, high=1.0, size=1)[0]
    else:
        vals_to_min = BIT_GEN.uniform(low=0.9, high=1.0, size=len(RES_FRAME))
        # vals_to_min[i - 190] = 0.0
        vals_to_min[i - 190] = BIT_GEN.uniform(low=0, high=0.1, size=1)[0]
else:
    if i < 190:
        vals_to_min = np.zeros(len(RES_FRAME))
        vals_to_min[i] = 1.0
    else:
        vals_to_min = np.ones(len(RES_FRAME))
        vals_to_min[i - 190] = 0.0

# variable for saving output
RESULT_FRAME_FILE = f'{OUT_FOLDER}/aa_{METHOD}_log{SERIAL}_{EXCL_FOLD}.csv'
if os.path.isfile(RESULT_FRAME_FILE) is False:
    RESULT_FRAME = pd.DataFrame(data=np.array([f'{x}{y}' for x, y in COMBS]),
                                columns=['ij']).set_index('ij').transpose()
    RESULT_FRAME.loc[0] = vals_to_min
    RESULT_FRAME.to_csv(RESULT_FRAME_FILE, index=False)  # output table with initial vals only if not exist

print(f'Using method: {METHOD.upper()}')
if os.path.isfile(f'{OUT_FOLDER}/aa_{METHOD}_cp{SERIAL}_{EXCL_FOLD}_optim.csv') is True:
    sys.exit(f'Set {SERIAL} already optimised')

# start optimiser
success_bool = False
while success_bool == False:
    if METHOD.lower() == 'cobyla':
        # FOR COBYLA: ineq means >= 0
        cons = ({'type': 'ineq',
                'fun': lambda W: 1 - W},
                {'type': 'ineq',
                'fun': lambda W: W})
        result = minimize(CalcROC,
                        vals_to_min,
                        constraints=cons,
                        method='cobyla',
                        options={'rhobeg': 0.2,
                                'disp': True,
                                'maxiter': MAX_ITER})
    elif METHOD.lower() == 'nelder-mead':
        result = minimize(CalcROC,
                        vals_to_min,
                        bounds=[(0, 1) for _ in range(len(RES_FRAME))],
                        method='nelder-mead',
                        options={'disp': True,
                                'maxiter': MAX_ITER,
                                'xatol': 0.0001,  # NOTE: threshold size of the simplex
                                #  'fatol': 0.0001,  # NOTE: threshold dist btwn vertices of simplex
                                'adaptive': False,  # NOTE: optimised for high-dimensionality?
                                'initial_simplex': prev_simplex})  # overrides x0 if not None
    
    # serialise results as json and save
    result_dict = dict(result)
    result_dict['success'] = str(result_dict['success'])
    result_dict = {k: v.tolist() if type(v) is np.ndarray
                else v for k, v in result_dict.items()}
    result_dict['final_simplex'] = [v.tolist() for v in result_dict.get('final_simplex', np.array([]))]
    # print(result_dict)
    
    with open(f'{OUT_FOLDER}/aa_{METHOD}_result{SERIAL}_{EXCL_FOLD}.json', 'w') as file:
        json.dump(result_dict, file, indent=4)
    
    # append current state
    with open(RESULT_FRAME_FILE, 'a') as f:
        f.write('\n')
        f.write(', '.join(map(str, result.x)))

    success_bool = result.success
    prev_simplex = result.final_simplex[0]
    print('MAX_ITER reached, rebooting')

# save base CP scores
outfile = CalcCP(vals_to_min)
outfile.to_csv(f'{OUT_FOLDER}/aa_{METHOD}_cp{SERIAL}_{EXCL_FOLD}.csv', index=False)

# save optimised CP scores
outfile = CalcCP(result_dict['x'])
outfile.to_csv(f'{OUT_FOLDER}/aa_{METHOD}_cp{SERIAL}_{EXCL_FOLD}_optim.csv', index=False)

# save test fold wAUC
if EXCL_FOLD <= 4:
    with open(f'{OUT_FOLDER}/aa_{METHOD}_log{SERIAL}_{EXCL_FOLD}_testfold.txt', 'w') as file:
        file.write(f'{100 - CalcROC(outfile)}')
